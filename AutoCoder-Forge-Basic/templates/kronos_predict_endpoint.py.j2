```project 1\AutoCoder-Forge-Basic\templates\kronos_predict_endpoint.py.j2
"""
Kronos Prediction API Router for FinAI Hub
Generated by AutoCoder

Template variables:
  - endpoint_prefix: str (API prefix)
  - model_path: str (Hugging Face model path)
  - tokenizer_path: str (Hugging Face tokenizer path)
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any
import pandas as pd
from datetime import datetime
import torch

# Import Kronos components (assuming installed)
try:
    from model import Kronos, KronosTokenizer, KronosPredictor
except ImportError:
    Kronos = None
    KronosTokenizer = None
    KronosPredictor = None

router = APIRouter(prefix="{{ endpoint_prefix }}", tags=["kronos"])

# Global predictor instance
predictor = None

class PredictionRequest(BaseModel):
    historical_data: List[Dict[str, Any]]  # List of OHLCV data points
    prediction_length: int = 120
    temperature: float = 1.0
    top_p: float = 0.9
    sample_count: int = 1

class PredictionResponse(BaseModel):
    predictions: List[Dict[str, Any]]
    metadata: Dict[str, Any]

@router.on_event("startup")
async def startup_event():
    global predictor
    if Kronos and KronosTokenizer and KronosPredictor:
        try:
            tokenizer = KronosTokenizer.from_pretrained("{{ tokenizer_path }}")
            model = Kronos.from_pretrained("{{ model_path }}")
            predictor = KronosPredictor(model, tokenizer, device="cuda:0" if torch.cuda.is_available() else "cpu")
        except Exception as e:
            print(f"Failed to load Kronos: {e}")
    else:
        print("Kronos dependencies not available")

@router.post("/predict", response_model=PredictionResponse)
async def predict_market(request: PredictionRequest):
    if not predictor:
        raise HTTPException(status_code=503, detail="Kronos predictor not available")

    try:
        # Convert historical data to DataFrame
        df = pd.DataFrame(request.historical_data)
        required_cols = ['open', 'high', 'low', 'close']
        if not all(col in df.columns for col in required_cols):
            raise HTTPException(status_code=400, detail="Missing required columns: open, high, low, close")

        # Prepare timestamps (assume index is timestamp)
        x_timestamp = pd.to_datetime(df.index)
        lookback = len(df)
        pred_len = request.prediction_length
        y_timestamp = pd.date_range(start=x_timestamp.iloc[-1], periods=pred_len+1, freq='D')[1:]

        # Make prediction
        pred_df = predictor.predict(
            df=df,
            x_timestamp=x_timestamp,
            y_timestamp=y_timestamp,
            pred_len=pred_len,
            T=request.temperature,
            top_p=request.top_p,
            sample_count=request.sample_count
        )

        # Convert to response format
        predictions = pred_df.to_dict('records')
        metadata = {
            "lookback": lookback,
            "prediction_length": pred_len,
            "model": "{{ model_path }}",
            "generated_at": datetime.utcnow().isoformat()
        }

        return PredictionResponse(predictions=predictions, metadata=metadata)

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")

@router.get("/status")
async def get_status():
    return {
        "available": predictor is not None,
        "model": "{{ model_path }}" if predictor else None,
        "tokenizer": "{{ tokenizer_path }}" if predictor else None
    }
